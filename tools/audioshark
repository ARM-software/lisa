#! /usr/bin/env python3

import argparse

import numpy as np
from scipy.io import wavfile
from scipy import signal

from lisa.trace import Trace, TaskID

def main():
    parser = argparse.ArgumentParser(
        description="""
        Turn a trace.dat into a wav file for the activation of each selected task.

        The pitch is mapped to the CPU (CPU 0 is the lowest note). The
        loundness is related to the duty cycle over one activation and its
        following sleep time.
        """
    )
    parser.add_argument('trace', help='trace.dat file')
    parser.add_argument('--duration', type=int, help='Duration of the audio file')
    parser.add_argument('--note', type=int,
        help='Musical note to use for CPU 0, in Hz',
        default=110,
    )
    task_parser = parser.add_mutually_exclusive_group(required=True)
    task_parser.add_argument('--rtapp',
        action='store_true',
        help='Detect rt-app tasks and select them. This relies on "rtapp_loop" rtapp ftrace event',
    )

    task_parser.add_argument('--task',
        default=[],
        action='append',
        help='PID or comm to select',
    )

    args = parser.parse_args()

    path = args.trace
    events = ['sched_switch', 'rtapp_loop', 'sched_wakeup']
    trace = Trace(path, events=events, normalize_time=True)

    def make_notes(start, nr_notes):
        x = start
        for _ in range(nr_notes):
            yield x
            x *= 2

    cpus_count = trace.cpus_count
    notes = list(make_notes(args.note, cpus_count))
    cpu_audio_freq = {
        cpu: notes[cpu]
        for cpu in range(cpus_count)
    }

    # Make sure to sample fast enough to reproduce the freqs we need
    sampling_rate = 44100
    max_audio_freq = max(cpu_audio_freq.values())
    if sampling_rate < 2 * max_audio_freq:
        sampling_rate = max_audio_freq * 5

    if args.rtapp:
        tasks = trace.analysis.rta.rtapp_tasks
    else:
        def to_task_id(task):
            try:
                pid = int(task)
            except ValueError:
                return TaskID(pid=None, comm=task)
            else:
                return TaskID(pid=pid, comm=None)

        tasks = [
            to_task_id(task)
            for task in args.task
        ]

    for task in tasks:
        df = trace.analysis.tasks.df_task_activation(task)
        track_duration = args.duration or (df.index[-1] - df.index[0])

        df.index = df.index * track_duration / df.index[-1]
        df['audio_freq'] = df['cpu'].map(cpu_audio_freq)

        # Map the duty cycle to an amplitude with a log, to match better human
        # ear capabilities
        df['amplitude'] = np.log10(df['duty_cycle'] * 9 + 1)

        for col in ('duty_cycle', 'amplitude'):
            for i in (0, 1):
                df[col].iat[i] = 1

        # Remove unnecessary transitions that would only add some clicks
        amplitude_d = df['amplitude'].diff()
        cpu_d = df['cpu'].diff()
        df = df[
            (cpu_d != 0) |
            # TODO: Find a better filter, as this will hide gradual ramps with
            # low slope even if they end up ramping a lot
            (amplitude_d.abs() > 0.05)
        ]
        df['audio_duration'] = df.index.to_series().diff().shift(-1)
        df['audio_duration'].iat[-1] = track_duration - df.index[-1]

        synthesize = make_synthetizer(
            sampling_rate=sampling_rate,
        )
        audio_data = np.concatenate([
            synthesize(row)
            for row in df.iterrows()
        ])

        # Smooth the output a bit with a Butterworth filter
        cutoff = max(cpu_audio_freq.values())
        nyquist_freq = sampling_rate / 2
        cutoff = cutoff / nyquist_freq
        b, a = signal.butter(5, cutoff)
        audio_data = signal.filtfilt(b, a, audio_data)

        filename = '{}.wav'.format(task.comm)
        print(filename)
        wavfile.write(filename, int(sampling_rate), audio_data)

def make_synthetizer(sampling_rate):
    prev_level = 0
    is_prev_rising = True
    prev_amplitude = 1

    def synthesize(index_row):
        nonlocal prev_level, is_prev_rising, prev_amplitude

        index, row = index_row
        freq = row['audio_freq']
        duration = row['audio_duration']
        amplitude = row['amplitude']

        sample_duration = 1 / sampling_rate
        support = np.arange(0, duration, sample_duration)

        phase = np.arcsin(prev_level if abs(prev_level) < 0.99 else 0.99)
        if not is_prev_rising:
            phase = np.pi - phase

        def make_sine(freq, phase):
            w = 2 * np.pi * freq

            # Achieve a transition as smooth as possible by matching the phase of
            # the new signal piece with the phase of the last sample of the
            # previous piece. This avoids steep transition, thereby reducing the
            # high frequency "clicks"
            return np.sin(phase + w * support)

        # Use mutliple harmonics, which is a tad more pleasing to the ear than a pure frequency
        harmonic_amplitudes = [
            1.0, 0.286699025, 0.150079537, 0.042909002,
            0.203797365, 0.229228698, 0.156931925,
            0.115470898, 0.0, 0.097401803, 0.087653465,
            0.052331036, 0.052922462, 0.038850593,
            0.053554676, 0.053697434, 0.022270261,
            0.013072562, 0.008585879, 0.005771505,
            0.004343925, 0.002141371, 0.005343231,
            0.000530244, 0.004711017, 0.009014153
        ]
        harmonic_amplitudes = harmonic_amplitudes[:5]

        def make_harmonics(fundamental, amplitudes):
            for i, amplitude in enumerate(amplitudes):
                yield i * fundamental, amplitude

        audio_signal = sum(
            amplitude * make_sine(freq, phase)
            for freq, amplitude in make_harmonics(freq, harmonic_amplitudes)
        )

        # Smooth the transition with a ramp and then a plateau
        support_duration = len(support) * sample_duration
        transition_duration = support_duration / 2
        # Cap the transition in seconds
        transition_duration = min(transition_duration, 0.2)
        transition_len = int(transition_duration // sample_duration)

        plateau_len = len(support) - transition_len
        envelop = np.concatenate([
            np.linspace(prev_amplitude, amplitude, transition_len),
            np.full(plateau_len, amplitude)
        ])
        audio_signal *= envelop

        prev_level = audio_signal[-1]
        try:
            is_prev_rising = audio_signal[-1] > audio_signal[-2]
        except IndexError:
            is_prev_rising = True
        prev_amplitude = amplitude
        return audio_signal

    return synthesize

if __name__ == '__main__':
    main()
